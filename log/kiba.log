Epoch: 950 train loss: 1.812903 train mse: 0.002354 test mse: 0.122538 ci: 0.901682 rm2: 0.809262
Namespace(device='cuda:0', epochs=1000, dataset='kiba', batch_size=512, learning_rate=0.002, lambda_1=1e-05, lambda_2=1, sim_type='sis', weight_decay=0.0, unit=0.1)
Epoch: 980 train loss: 1.840690 train mse: 0.001895 test mse: 0.126045 ci: 0.899768 rm2: 0.802172
Namespace(device='cuda:0', epochs=1000, dataset='kiba', batch_size=512, learning_rate=0.002, lambda_1=1e-05, lambda_2=1, sim_type='cosine', weight_decay=0.0, unit=0.1)
Epoch: 930 train loss: 2.079152 train mse: 0.004109 test mse: 0.124397 ci: 0.899736 rm2: 0.802986
Namespace(device='cuda:0', epochs=1000, dataset='kiba', batch_size=512, learning_rate=0.002, lambda_1=1e-05, lambda_2=1, sim_type='pearson', weight_decay=0.0, unit=0.1)
Epoch: 1000 train loss: 1.900462 train mse: 0.002110 test mse: 0.124648 ci: 0.899926 rm2: 0.799026
Namespace(device='cuda:0', epochs=1000, dataset='kiba', batch_size=512, learning_rate=0.002, lambda_1=1e-05, lambda_2=1, sim_type='euclidean', weight_decay=0.0, unit=0.1)
Epoch: 980 train loss: 1.851485 train mse: 0.003757 test mse: 0.124302 ci: 0.897631 rm2: 0.800419
Namespace(device='cuda:0', epochs=1000, dataset='kiba', batch_size=512, learning_rate=0.002, lambda_1=1e-05, lambda_2=1, sim_type='jaccard', weight_decay=0.0, unit=0.1)
Epoch: 1000 train loss: 1.992192 train mse: 0.005730 test mse: 0.135229 ci: 0.890461 rm2: 0.779681
Namespace(device='cuda:0', epochs=1000, dataset='kiba', batch_size=512, learning_rate=0.002, lambda_1=1e-05, lambda_2=1, sim_type='default', weight_decay=0.0, unit=0.1)